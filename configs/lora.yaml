model_name: gpt2
dataset_path: data/sample_corpus.txt
output_dir: output/gpt2_lora_finetuned

peft_config:
  lora_alpha: 16
  lora_dropout: 0.1
  r: 8
  bias: "none"
  task_type: "CAUSAL_LM"

training_args:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 1
  learning_rate: 2e-4
  weight_decay: 0.01
  save_steps: 500
  logging_steps: 50
  save_total_limit: 2
  fp16: True
  push_to_hub: False

gen_max_length: 100
gen_num_return_sequences: 3